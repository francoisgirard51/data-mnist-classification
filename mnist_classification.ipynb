{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üéØ <b><u>Exercise objectives</u></b>\n",
    "- Understand the *MNIST* dataset \n",
    "- Design your first **Convolutional Neural Network** (*CNN*) and answer questions such as:\n",
    "    - what are *Convolutional Layers*? \n",
    "    - how many *parameters* are involved in such a layer?\n",
    "- Train this CNN on images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üöÄ <b><u>Let's get started!</u></b>\n",
    "\n",
    "Imagine that we are  back in time into the 90's.\n",
    "You work at a *Post Office* and you have to deal with an enormous amount of letters on a daily basis. How could you automate the process of reading the ZIP Codes, which are a combination of 5 handwritten digits? \n",
    "\n",
    "This task, called the **Handwriting Recognition**, used to be a very complex problem back in those days. It was solved by *Bell Labs* (among others) where one of the Deep Learning gurus, [*Yann Le Cun*](https://en.wikipedia.org/wiki/Yann_LeCun), used to work.\n",
    "\n",
    "From [Wikipedia](https://en.wikipedia.org/wiki/Handwriting_recognition):\n",
    "\n",
    "> Handwriting recognition (HWR), also known as Handwritten Text Recognition (HTR), is the ability of a computer to receive and interpret intelligible handwritten input from sources such as paper documents, photographs, touch-screens and other devices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Number recognition](recognition.gif)\n",
    "\n",
    "*Note: The animation above is just here to help you visualize what happens with the different images: <br/> $\\rightarrow$ For each image, once the CNN is trained, it will predict what digit is written. The inputs are the different digits and not one animation/video!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ü§î <b><u>How does this CNN work ?</u></b>\n",
    "\n",
    "- *Inputs*: Images (_each image shows a handwritten digit_)\n",
    "- *Target*: For each image, you want your CNN model to predict the correct digit (between 0 and 9)\n",
    "    - It is a **multi-class classification** task (more precisely a 10-class classification task since there are 10 different digits).\n",
    "\n",
    "üî¢ To improve the capacity of the Convolutional Neural Network to read these numbers, we need to feed it with many images representing handwritten digits. This is why the üìö [**MNIST dataset**](http://yann.lecun.com/exdb/mnist/) *(Mixed National Institute of Standards and Technology)* was created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) The `MNIST` Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìö Tensorflow/Keras offers multiple [**datasets**](https://www.tensorflow.org/api_docs/python/tf/keras/datasets) to play with:\n",
    "- *Vectors*: `boston_housing` (regression)\n",
    "- *Images* : `mnist`, `fashion_mnist`, `cifar10`, `cifar100` (classification)\n",
    "- *Texts*: `imbd`, `reuters` (classification/sentiment analysis)\n",
    "\n",
    "\n",
    "üíæ You can **load the MNIST dataset** with the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-22 18:14:56.292630: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 2s 0us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(((60000, 28, 28), (60000,)), ((10000, 28, 28), (10000,)))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras import datasets\n",
    "\n",
    "\n",
    "# Loading the MNIST Dataset...\n",
    "(X_train, y_train), (X_test, y_test) = datasets.mnist.load_data(path=\"mnist.npz\")\n",
    "\n",
    "# The train set contains 60 000 images, each of them of size 28x28\n",
    "# The test set contains 10 000 images, each of them of size 28x28\n",
    "(X_train.shape, y_train.shape), (X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.1) Exploring the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: Let's have look at some handwritten digits of this MNIST dataset.** ‚ùì\n",
    "\n",
    "üñ® Print some images from the *train set*.\n",
    "\n",
    "<details>\n",
    "    <summary><i>Hints</i></summary>\n",
    "\n",
    "üí°*Hint*: use the `imshow` function from `matplotlib` with `cmap = \"gray\"`\n",
    "\n",
    "ü§® Note: if you don't specify this *cmap* argument, the weirdly displayed colors are just Matplotlib defaults...\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 900x900 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Number of images to display\n",
    "num_images = 9  # For a 3x3 grid\n",
    "plt.figure(figsize=(9, 9))  # Setting the figure size\n",
    "\n",
    "for i in range(num_images):\n",
    "    # Plot in a 3x3 grid\n",
    "    plt.subplot(3, 3, i + 1)\n",
    "    \n",
    "    # Select the i-th image from X_train and display it\n",
    "    plt.imshow(X_train[i], cmap='gray')\n",
    "    \n",
    "    # Turn off axis labels\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Add title with the label\n",
    "    plt.title(f'Label: {y_train[i]}')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.2) Image Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùóÔ∏è **Neural Networks converge faster when the input data is somehow normalized** ‚ùóÔ∏è\n",
    "\n",
    "üë©üèª‚Äçüè´ How do we proceed for Convolutional Neural Networks ?\n",
    "* The `RBG` intensities are coded between 0 and 255. \n",
    "* We can simply divide the input data by the maximal value 255 to have all the pixels' intensities between 0 and 1 üòâ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question ‚ùì As a first preprocessing step, please normalize your data.** \n",
    "\n",
    "Don't forget to do it both on your train data and your test data.\n",
    "\n",
    "(*Note: you can also center your data, by subtracting 0.5 from all the values, but it is not mandatory*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# Normalize the training data\n",
    "X_train_normalized = X_train / 255.0\n",
    "\n",
    "# Normalize the test data\n",
    "X_test_normalized = X_test / 255.0\n",
    "\n",
    "# Optionally, you can also center the data by subtracting 0.5\n",
    "# X_train_normalized = (X_train / 255.0) - 0.5\n",
    "# X_test_normalized = (X_test / 255.0) - 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.3) Inputs' dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëÜ Remember that you have 60,000 training images and 10,000 test images, each of size $(28, 28)$. However...\n",
    "\n",
    "> ‚ùóÔ∏è  **`Convolutional Neural Network models need to be fed with images whose last dimension is the number of channels`.**  \n",
    "\n",
    "> üßëüèª‚Äçüè´ The shape of tensors fed into ***ConvNets*** is the following: `(NUMBER_OF_IMAGES, HEIGHT, WIDTH, CHANNELS)`\n",
    "\n",
    "üïµüèªThis last dimension is clearly missing here. Can you guess the reason why?\n",
    "<br>\n",
    "<details>\n",
    "    <summary><i>Answer<i></summary>\n",
    "        \n",
    "* All these $60000$ $ (28 \\times 28) $ pictures are black-and-white $ \\implies $ Each pixel lives on a spectrum from full black (0) to full white (1).\n",
    "        \n",
    "    * Theoretically, you don't need to know the number of channels for a black-and-white picture since there is only 1 channel (the \"whiteness\" of \"blackness\" of a pixel). However, it is still mandatory for the model to have this number of channels explicitly stated.\n",
    "        \n",
    "    * In comparison, colored pictures need multiple channels:\n",
    "        - the RGB system with 3 channels (<b><span style=\"color:red\">Red</span> <span style=\"color:green\">Green</span> <span style=\"color:blue\">Blue</span></b>)\n",
    "        - the CYMK system  with 4 channels (<b><span style=\"color:cyan\">Cyan</span> <span style=\"color:magenta\">Magenta</span> <span style=\"color:yellow\">Yellow</span> <span style=\"color:black\">Black</span></b>)\n",
    "        \n",
    "        \n",
    "</details>        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: expanding dimensions** ‚ùì\n",
    "\n",
    "* Use the **`expand_dims`** to add one dimension at the end of the training data and test data.\n",
    "\n",
    "* Then, print the shapes of `X_train` and `X_test`. They should respectively be equal to $(60000, 28, 28, 1)$ and $(10000, 28, 28, 1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.backend import expand_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (60000, 28, 28, 1)\n",
      "Shape of X_test: (10000, 28, 28, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-22 18:22:24.786555: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Add an extra dimension to the training and test data\n",
    "X_train_expanded = expand_dims(X_train_normalized, axis=-1)\n",
    "X_test_expanded = expand_dims(X_test_normalized, axis=-1)\n",
    "\n",
    "# Print the shapes to verify\n",
    "print(\"Shape of X_train:\", X_train_expanded.shape)\n",
    "print(\"Shape of X_test:\", X_test_expanded.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.4) Target encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more thing to for a multiclass classification task in Deep Leaning:\n",
    "\n",
    "üëâ _\"one-hot-encode\" the categories*_\n",
    "\n",
    "‚ùì **Question: encoding the labels** ‚ùì \n",
    "\n",
    "* Use **`to_categorical`** to transform your labels. \n",
    "* Store the results into two variables that you can call **`y_train_cat`** and **`y_test_cat`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y_train_cat: (60000, 10)\n",
      "Shape of y_test_cat: (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "y_train_cat = to_categorical(y_train)\n",
    "y_test_cat = to_categorical(y_test)\n",
    "\n",
    "# Check the shape of the encoded labels\n",
    "print(\"Shape of y_train_cat:\", y_train_cat.shape)\n",
    "print(\"Shape of y_test_cat:\", y_test_cat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick check that you correctly used to_categorical\n",
    "assert(y_train_cat.shape == (60000,10))\n",
    "assert(y_test_cat.shape == (10000,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is now ready to be used. ‚úÖ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) The Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.1) Architecture and compilation of a CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "‚ùì **Question: CNN Architecture and compilation** ‚ùì\n",
    "\n",
    "Now, let's build a <u>Convolutional Neural Network</u> that has: \n",
    "\n",
    "\n",
    "- a `Conv2D` layer with 8 filters, each of size $(4, 4)$, an input shape suitable for your task, the `relu` activation function, and `padding='same'`\n",
    "- a `MaxPool2D` layer with a `pool_size` equal to $(2, 2)$\n",
    "- a second `Conv2D` layer with 16 filters, each of size $(3, 3)$, and the `relu` activation function\n",
    "- a second `MaxPool2D` layer with a `pool_size` equal to $(2, 2)$\n",
    "\n",
    "\n",
    "- a `Flatten` layer\n",
    "- a first `Dense` layer with 10 neurons and the `relu` activation function\n",
    "- a last (predictive) layer that is suited for your task\n",
    "\n",
    "In the function that initializes this model, do not forget to include the <u>compilation of the model</u>, which:\n",
    "* optimizes the `categorical_crossentropy` loss function,\n",
    "* with the `adam` optimizer, \n",
    "* and the `accuracy` as the metrics\n",
    "\n",
    "(*Note: you could add more classification metrics if you want but the dataset is well balanced!*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 28, 28, 8)         136       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 14, 14, 8)        0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 12, 12, 16)        1168      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 6, 6, 16)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 576)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                5770      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                110       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,184\n",
      "Trainable params: 7,184\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "\n",
    "\n",
    "def initialize_model():\n",
    "\n",
    "    model = models.Sequential()\n",
    "\n",
    "    # First Convolution & MaxPooling\n",
    "    model.add(layers.Conv2D(8, (4, 4), activation='relu', padding='same', input_shape=(28, 28, 1)))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "    # Second Convolution & MaxPooling\n",
    "    model.add(layers.Conv2D(16, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "    # Flattening\n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    # One Fully Connected layer\n",
    "    model.add(layers.Dense(10, activation='relu'))\n",
    "\n",
    "    # Last layer - Classification Layer with 10 outputs\n",
    "    model.add(layers.Dense(10, activation='softmax'))  # 10 classes for MNIST digits\n",
    "\n",
    "    # Model compilation\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Initialize the model\n",
    "model = initialize_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: number of trainable parameters in a convolutional layer** ‚ùì \n",
    "\n",
    "How many trainable parameters are there in your model?\n",
    "1. Compute them with ***model.summary( )*** first\n",
    "2. Recompute them manually to make sure you properly understood ***what influences the number of weights in a CNN***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.2) Training a CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: training a CNN** ‚ùì \n",
    "\n",
    "Initialize your model and fit it on the train data. \n",
    "- Do not forget to use a **Validation Set/Split** and an **Early Stopping criterion**. \n",
    "- Limit yourself to 5 epochs max in this challenge, just to save some precious time for the more advanced challenges!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1500/1500 [==============================] - 29s 19ms/step - loss: 0.3615 - accuracy: 0.8894 - val_loss: 0.1323 - val_accuracy: 0.9605\n",
      "Epoch 2/5\n",
      "1500/1500 [==============================] - 25s 17ms/step - loss: 0.1173 - accuracy: 0.9651 - val_loss: 0.1018 - val_accuracy: 0.9718\n",
      "Epoch 3/5\n",
      "1500/1500 [==============================] - 29s 19ms/step - loss: 0.0901 - accuracy: 0.9730 - val_loss: 0.0888 - val_accuracy: 0.9735\n",
      "Epoch 4/5\n",
      "1499/1500 [============================>.] - ETA: 0s - loss: 0.0756 - accuracy: 0.9772"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Initialize the model\n",
    "model = initialize_model()\n",
    "\n",
    "# Define the EarlyStopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=1)\n",
    "\n",
    "# Fit the model on the training data\n",
    "history = model.fit(\n",
    "    X_train_expanded,\n",
    "    y_train_cat,\n",
    "    epochs=5,  # Limit to 5 epochs\n",
    "    validation_split=0.2,  # Use 20% of the training data for validation\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# Optional: Plot the training history\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0, 1])\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: How many iterations does the CNN perform per epoch** ‚ùì\n",
    "\n",
    "_Note: it has nothing to do with the fact that this is a CNN. This is related to the concept of forward/backward propagation already covered during the previous lecture on optimizers, fitting, and losses üòâ_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "source": [
    "> YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><i>Answer</i></summary>\n",
    "\n",
    "With `verbose = 1` when fitting your model, you have access to crucial information about your training procedure.\n",
    "    \n",
    "Remember that we've just trained our CNN model on $60000$ training images\n",
    "\n",
    "If the chosen batch size is 32: \n",
    "\n",
    "* For each epoch, we have $ \\large \\lceil \\frac{60000}{32} \\rceil = 1875$ minibatches <br/>\n",
    "* The _validation_split_ is equal to $0.3$ - which means that within one single epoch, there are:\n",
    "    * $ \\lceil 1875 \\times (1 - 0.3) \\rceil = \\lceil 1312.5 \\rceil = 1313$ batches are used to compute the `train_loss` \n",
    "    * $ 1875 - 1312 = 562 $ batches are used to compute the `val_loss`\n",
    "    * **The parameters are updated 1313 times per epoch** as there are 1313 forward/backward propagations per epoch !!!\n",
    "\n",
    "\n",
    "üëâ With so many updates of the weights within one epoch, you can understand why this CNN model converges even with a limited number of epochs.\n",
    "\n",
    "</details>    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.3) Evaluating its performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: Evaluating your CNN** ‚ùì \n",
    "\n",
    "What is your **`accuracy on the test set?`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üéâ You should already be impressed by your CNN skills! Reaching over 95% accuracy!\n",
    "\n",
    "üî• You solved what was a very hard problem 30 years ago with your own CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üèÅ **Congratulations!**\n",
    "\n",
    "üíæ Don't forget to `git add/commit/push` your notebook...\n",
    "\n",
    "üöÄ ... and move on to the next challenge!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
